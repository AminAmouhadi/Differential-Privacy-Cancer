{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eiwVljWpzM7"
   },
   "source": [
    "Copyright 2020 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "4rmwPgXeptiS"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YM2gRaJMqvMi"
   },
   "source": [
    "# Assess privacy risks with TensorFlow Privacy Membership Inference Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B5ZvlSqqLaR"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/membership_inference_attack/codelabs/codelab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/membership_inference_attack/codelabs/codelab.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rMuytY7Nn8P"
   },
   "source": [
    "##Overview\n",
    "In this codelab we'll train a simple image classification model on the CIFAR10 dataset, and then use the \"membership inference attack\" against this model to assess if the attacker is able to \"guess\" whether a particular sample was present in the training set. We further compute each sample's probability of being in the training set, denoted as the privacy risk score (https://arxiv.org/abs/2003.10595)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUWqArj_q8vs"
   },
   "source": [
    "## Setup\n",
    "First, set this notebook's runtime to use a GPU, under Runtime > Change runtime type > Hardware accelerator. Then, begin importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Lr1pwHcbralz"
   },
   "outputs": [],
   "source": [
    "#@title Import statements.\n",
    "import numpy as np\n",
    "from typing import Tuple, Text\n",
    "from scipy import special\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Set verbosity.\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucw81ar6ru-6"
   },
   "source": [
    "### Install TensorFlow Privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "zcqAmiGH90kl"
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/tensorflow/privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.membership_inference_attack import membership_inference_attack as mia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBbcG86th_sW"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "vCyOWyyhXLib"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset.\n",
      "learning rate %f 0.02\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,298\n",
      "Trainable params: 28,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 2.0612 - accuracy: 0.2296 - val_loss: 1.7651 - val_accuracy: 0.3543\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.6134 - accuracy: 0.4093 - val_loss: 1.4970 - val_accuracy: 0.4533\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.4312 - accuracy: 0.4798 - val_loss: 1.3316 - val_accuracy: 0.5254\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 1.3117 - accuracy: 0.5308 - val_loss: 1.2511 - val_accuracy: 0.5495\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.2103 - accuracy: 0.5715 - val_loss: 1.1634 - val_accuracy: 0.5906\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.1535 - accuracy: 0.5943 - val_loss: 1.1286 - val_accuracy: 0.6007\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.0921 - accuracy: 0.6147 - val_loss: 1.1256 - val_accuracy: 0.6005\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.0472 - accuracy: 0.6319 - val_loss: 1.0726 - val_accuracy: 0.6266\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.0173 - accuracy: 0.6434 - val_loss: 1.0692 - val_accuracy: 0.6228\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.9775 - accuracy: 0.6543 - val_loss: 0.9961 - val_accuracy: 0.6544\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.9361 - accuracy: 0.6747 - val_loss: 0.9996 - val_accuracy: 0.6519\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.9077 - accuracy: 0.6815 - val_loss: 1.0096 - val_accuracy: 0.6526\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8918 - accuracy: 0.6865 - val_loss: 0.9685 - val_accuracy: 0.6624\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8501 - accuracy: 0.7016 - val_loss: 0.9513 - val_accuracy: 0.6737\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8402 - accuracy: 0.7041 - val_loss: 1.0338 - val_accuracy: 0.6449\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8218 - accuracy: 0.7099 - val_loss: 0.9843 - val_accuracy: 0.6588\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8029 - accuracy: 0.7167 - val_loss: 0.9515 - val_accuracy: 0.6724\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7858 - accuracy: 0.7230 - val_loss: 0.9279 - val_accuracy: 0.6790\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7609 - accuracy: 0.7316 - val_loss: 0.9664 - val_accuracy: 0.6705\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7452 - accuracy: 0.7352 - val_loss: 0.9328 - val_accuracy: 0.6880\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7412 - accuracy: 0.7378 - val_loss: 0.9045 - val_accuracy: 0.6869\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7174 - accuracy: 0.7468 - val_loss: 0.9313 - val_accuracy: 0.6844\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7113 - accuracy: 0.7481 - val_loss: 0.9915 - val_accuracy: 0.6594\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6890 - accuracy: 0.7575 - val_loss: 0.9174 - val_accuracy: 0.6922\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6839 - accuracy: 0.7578 - val_loss: 0.9313 - val_accuracy: 0.6891\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6696 - accuracy: 0.7627 - val_loss: 0.9411 - val_accuracy: 0.6838\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6581 - accuracy: 0.7673 - val_loss: 0.9240 - val_accuracy: 0.6900\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6504 - accuracy: 0.7717 - val_loss: 0.9469 - val_accuracy: 0.6872\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6486 - accuracy: 0.7700 - val_loss: 0.9310 - val_accuracy: 0.6924\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6389 - accuracy: 0.7761 - val_loss: 0.9203 - val_accuracy: 0.6977\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6198 - accuracy: 0.7808 - val_loss: 0.9639 - val_accuracy: 0.6865\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6234 - accuracy: 0.7810 - val_loss: 0.9300 - val_accuracy: 0.6978\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6213 - accuracy: 0.7801 - val_loss: 0.9401 - val_accuracy: 0.6939\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5966 - accuracy: 0.7881 - val_loss: 0.9759 - val_accuracy: 0.6893\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5935 - accuracy: 0.7905 - val_loss: 0.9545 - val_accuracy: 0.6918\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5843 - accuracy: 0.7943 - val_loss: 0.9762 - val_accuracy: 0.6881\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5841 - accuracy: 0.7923 - val_loss: 0.9974 - val_accuracy: 0.6932\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5705 - accuracy: 0.8001 - val_loss: 0.9882 - val_accuracy: 0.6956\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5755 - accuracy: 0.7966 - val_loss: 1.0226 - val_accuracy: 0.6828\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5720 - accuracy: 0.7977 - val_loss: 1.0234 - val_accuracy: 0.6865\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5625 - accuracy: 0.7990 - val_loss: 1.0069 - val_accuracy: 0.6901\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5516 - accuracy: 0.8031 - val_loss: 1.0525 - val_accuracy: 0.6834\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5506 - accuracy: 0.8029 - val_loss: 0.9840 - val_accuracy: 0.6958\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5461 - accuracy: 0.8055 - val_loss: 1.0448 - val_accuracy: 0.6848\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5351 - accuracy: 0.8098 - val_loss: 1.0133 - val_accuracy: 0.6991\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s 9ms/step - loss: 0.5219 - accuracy: 0.8157 - val_loss: 1.0694 - val_accuracy: 0.6729\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5232 - accuracy: 0.8121 - val_loss: 1.0447 - val_accuracy: 0.6902\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5180 - accuracy: 0.8149 - val_loss: 1.0495 - val_accuracy: 0.6907\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5022 - accuracy: 0.8217 - val_loss: 1.0147 - val_accuracy: 0.6941\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5017 - accuracy: 0.8210 - val_loss: 1.0510 - val_accuracy: 0.6925\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4997 - accuracy: 0.8230 - val_loss: 1.0620 - val_accuracy: 0.6935\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.5001 - accuracy: 0.8214 - val_loss: 1.1083 - val_accuracy: 0.6832\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4976 - accuracy: 0.8213 - val_loss: 1.0512 - val_accuracy: 0.6924\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4858 - accuracy: 0.8250 - val_loss: 1.0825 - val_accuracy: 0.6936\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4859 - accuracy: 0.8241 - val_loss: 1.1278 - val_accuracy: 0.6763\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4839 - accuracy: 0.8275 - val_loss: 1.1310 - val_accuracy: 0.6900\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4805 - accuracy: 0.8295 - val_loss: 1.1174 - val_accuracy: 0.6838\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4836 - accuracy: 0.8278 - val_loss: 1.1538 - val_accuracy: 0.6884\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4816 - accuracy: 0.8278 - val_loss: 1.1088 - val_accuracy: 0.6831\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4589 - accuracy: 0.8351 - val_loss: 1.1430 - val_accuracy: 0.6827\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4633 - accuracy: 0.8337 - val_loss: 1.0974 - val_accuracy: 0.6935\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4562 - accuracy: 0.8362 - val_loss: 1.1591 - val_accuracy: 0.6809\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4604 - accuracy: 0.8340 - val_loss: 1.1722 - val_accuracy: 0.6778\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4536 - accuracy: 0.8378 - val_loss: 1.1837 - val_accuracy: 0.6740\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4773 - accuracy: 0.8281 - val_loss: 1.2039 - val_accuracy: 0.6746\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4444 - accuracy: 0.8400 - val_loss: 1.1762 - val_accuracy: 0.6784\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4452 - accuracy: 0.8404 - val_loss: 1.2105 - val_accuracy: 0.6767\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4444 - accuracy: 0.8387 - val_loss: 1.2040 - val_accuracy: 0.6763\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4482 - accuracy: 0.8383 - val_loss: 1.2023 - val_accuracy: 0.6807\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4482 - accuracy: 0.8388 - val_loss: 1.1758 - val_accuracy: 0.6891\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4323 - accuracy: 0.8447 - val_loss: 1.1958 - val_accuracy: 0.6883\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4402 - accuracy: 0.8399 - val_loss: 1.2304 - val_accuracy: 0.6834\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4259 - accuracy: 0.8479 - val_loss: 1.2637 - val_accuracy: 0.6711\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4319 - accuracy: 0.8444 - val_loss: 1.1960 - val_accuracy: 0.6826\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4332 - accuracy: 0.8420 - val_loss: 1.2422 - val_accuracy: 0.6868\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4207 - accuracy: 0.8477 - val_loss: 1.2335 - val_accuracy: 0.6802\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4284 - accuracy: 0.8452 - val_loss: 1.2575 - val_accuracy: 0.6802\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4129 - accuracy: 0.8502 - val_loss: 1.3147 - val_accuracy: 0.6734\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4300 - accuracy: 0.8448 - val_loss: 1.2489 - val_accuracy: 0.6795\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4190 - accuracy: 0.8490 - val_loss: 1.3098 - val_accuracy: 0.6658\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4192 - accuracy: 0.8475 - val_loss: 1.3094 - val_accuracy: 0.6794\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4179 - accuracy: 0.8474 - val_loss: 1.2586 - val_accuracy: 0.6812\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4101 - accuracy: 0.8509 - val_loss: 1.2885 - val_accuracy: 0.6766\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4076 - accuracy: 0.8521 - val_loss: 1.3107 - val_accuracy: 0.6728\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4095 - accuracy: 0.8510 - val_loss: 1.3321 - val_accuracy: 0.6797\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4051 - accuracy: 0.8535 - val_loss: 1.3349 - val_accuracy: 0.6755\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3985 - accuracy: 0.8536 - val_loss: 1.2849 - val_accuracy: 0.6760\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3933 - accuracy: 0.8576 - val_loss: 1.3214 - val_accuracy: 0.6799\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4005 - accuracy: 0.8537 - val_loss: 1.3200 - val_accuracy: 0.6793\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3939 - accuracy: 0.8561 - val_loss: 1.3327 - val_accuracy: 0.6755\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3904 - accuracy: 0.8565 - val_loss: 1.3969 - val_accuracy: 0.6770\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3989 - accuracy: 0.8554 - val_loss: 1.3437 - val_accuracy: 0.6761\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3921 - accuracy: 0.8578 - val_loss: 1.4248 - val_accuracy: 0.6763\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3781 - accuracy: 0.8609 - val_loss: 1.3771 - val_accuracy: 0.6728\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4045 - accuracy: 0.8528 - val_loss: 1.4156 - val_accuracy: 0.6735\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3998 - accuracy: 0.8536 - val_loss: 1.3608 - val_accuracy: 0.6770\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3875 - accuracy: 0.8587 - val_loss: 1.4172 - val_accuracy: 0.6642\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3975 - accuracy: 0.8537 - val_loss: 1.3898 - val_accuracy: 0.6758\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.3779 - accuracy: 0.8610 - val_loss: 1.3825 - val_accuracy: 0.6743\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3836 - accuracy: 0.8613 - val_loss: 1.4445 - val_accuracy: 0.6738\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "#@markdown Train a simple model on CIFAR10 with Keras.\n",
    "\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "num_conv = 3\n",
    "activation = 'relu'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "batch_size = 250\n",
    "epochs = 100  # Privacy risks are especially visible with lots of epochs.\n",
    "\n",
    "\n",
    "def small_cnn(input_shape: Tuple[int],\n",
    "              num_classes: int,\n",
    "              num_conv: int,\n",
    "              activation: Text = 'relu') -> tf.keras.models.Sequential:\n",
    "  \"\"\"Setup a small CNN for image classification.\n",
    "\n",
    "  Args:\n",
    "    input_shape: Integer tuple for the shape of the images.\n",
    "    num_classes: Number of prediction classes.\n",
    "    num_conv: Number of convolutional layers.\n",
    "    activation: The activation function to use for conv and dense layers.\n",
    "\n",
    "  Returns:\n",
    "    The Keras model.\n",
    "  \"\"\"\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "  # Conv layers\n",
    "  for _ in range(num_conv):\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation=activation))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(64, activation=activation))\n",
    "  model.add(tf.keras.layers.Dense(num_classes))\n",
    "  return model\n",
    "\n",
    "\n",
    "print('Loading the dataset.')\n",
    "train_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TRAIN, batch_size=-1))\n",
    "test_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TEST, batch_size=-1))\n",
    "x_train = train_ds['image'].astype('float32') / 255.\n",
    "y_train_indices = train_ds['label'][:, np.newaxis]\n",
    "x_test = test_ds['image'].astype('float32') / 255.\n",
    "y_test_indices = test_ds['label'][:, np.newaxis]\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train_indices, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_indices, num_classes)\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = small_cnn(\n",
    "    input_shape, num_classes, num_conv=num_conv, activation=activation)\n",
    "\n",
    "print('learning rate %f', lr)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=momentum)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True)\n",
    "print('Finished training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ee-zjGGGV1DC"
   },
   "source": [
    "## Calculate logits, probabilities and loss values for training and test sets.\n",
    "\n",
    "We will use these values later in the membership inference attack and privacy risk score analysis to separate training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "um9r0tSiPx4u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on train...\n",
      "Predict on test...\n",
      "Apply softmax to get probabilities from logits...\n",
      "Compute losses...\n"
     ]
    }
   ],
   "source": [
    "print('Predict on train...')\n",
    "logits_train = model.predict(x_train, batch_size=batch_size)\n",
    "print('Predict on test...')\n",
    "logits_test = model.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "print('Apply softmax to get probabilities from logits...')\n",
    "prob_train = special.softmax(logits_train, axis=1)\n",
    "prob_test = special.softmax(logits_test, axis=1)\n",
    "\n",
    "print('Compute losses...')\n",
    "cce = tf.keras.backend.categorical_crossentropy\n",
    "constant = tf.keras.backend.constant\n",
    "\n",
    "loss_train = cce(constant(y_train), constant(prob_train), from_logits=False).numpy()\n",
    "loss_test = cce(constant(y_test), constant(prob_test), from_logits=False).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QETxVOHLiHP4"
   },
   "source": [
    "## Run membership inference attacks.\n",
    "\n",
    "We will now execute a membership inference attack against the previously trained CIFAR10 model. This will generate a number of scores, most notably, attacker advantage and AUC for the membership inference classifier.\n",
    "\n",
    "An AUC of close to 0.5 means that the attack wasn't able to identify training samples, which means that the model doesn't have privacy issues according to this test. Higher values, on the contrary, indicate potential privacy issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8NIwhVwQT7I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing attacks over all slices\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.75 on slice CORRECTLY_CLASSIFIED=False\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.39 on slice CORRECTLY_CLASSIFIED=False\n",
      "\n",
      "Best-performing attacks over slice: \"Entire dataset\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.62\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.21\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=0\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.65\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.28\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=1\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.59\n",
      "  THRESHOLD_ENTROPY_ATTACK achieved an advantage of 0.18\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=2\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.72\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.33\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=3\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.68\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.30\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=4\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.68\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.28\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=5\"\n",
      "  THRESHOLD_ENTROPY_ATTACK achieved an AUC of 0.63\n",
      "  THRESHOLD_ENTROPY_ATTACK achieved an advantage of 0.23\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=6\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.59\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.19\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=7\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.62\n",
      "  THRESHOLD_ATTACK achieved an advantage of 0.21\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=8\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.59\n",
      "  THRESHOLD_ENTROPY_ATTACK achieved an advantage of 0.17\n",
      "\n",
      "Best-performing attacks over slice: \"CLASS=9\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.64\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.22\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.52\n",
      "  THRESHOLD_ATTACK achieved an advantage of 0.06\n",
      "\n",
      "Best-performing attacks over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  LOGISTIC_REGRESSION achieved an AUC of 0.75\n",
      "  LOGISTIC_REGRESSION achieved an advantage of 0.39\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackType\n",
    "\n",
    "import tensorflow_privacy.privacy.membership_inference_attack.plotting as plotting\n",
    "\n",
    "labels_train = np.argmax(y_train, axis=1)\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "input = AttackInputData(\n",
    "  logits_train = logits_train,\n",
    "  logits_test = logits_test,\n",
    "  loss_train = loss_train,\n",
    "  loss_test = loss_test,\n",
    "  labels_train = labels_train,\n",
    "  labels_test = labels_test\n",
    ")\n",
    "\n",
    "# Run several attacks for different data slices\n",
    "attacks_result = mia.run_attacks(input,\n",
    "                                 SlicingSpec(\n",
    "                                     entire_dataset = True,\n",
    "                                     by_class = True,\n",
    "                                     by_classification_correctness = True\n",
    "                                 ),\n",
    "                                 attack_types = [\n",
    "                                     AttackType.THRESHOLD_ATTACK,\n",
    "                                     AttackType.THRESHOLD_ENTROPY_ATTACK,\n",
    "                                     AttackType.LOGISTIC_REGRESSION])\n",
    "\n",
    "# Plot the ROC curve of the best classifier\n",
    "fig = plotting.plot_roc_curve(\n",
    "    attacks_result.get_result_with_max_auc().roc_curve)\n",
    "\n",
    "# Print a user-friendly summary of the attacks\n",
    "print(attacks_result.summary(by_slices = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9zwsPGFujVq"
   },
   "source": [
    "## Compute privacy risk score\n",
    "\n",
    "This part shows how to use the privacy risk score.\n",
    "\n",
    "For each data slice, we compute privacy risk scores for both training and test data. We then set a threshold on risk scores (an input is inferred as a member if and only if its risk score is higher than the threshold) and compute the attack precision and recall values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Privacy risk score analysis over slice: \"Entire dataset\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.60966, 0.10730)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.56588, 0.87102)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=0\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.62251, 0.26880)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.57677, 0.74680)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=1\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.61579, 0.23560)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.58356, 0.64880)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=2\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.64815, 0.58580)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.62353, 0.80660)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=3\"\n",
      "  with 1.00000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00060)\n",
      "  with 0.90000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00060)\n",
      "  with 0.80000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00060)\n",
      "  with 0.70000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00060)\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.62839, 0.60200)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.59257, 0.90320)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=4\"\n",
      "  with 1.00000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00140)\n",
      "  with 0.90000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00140)\n",
      "  with 0.80000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00140)\n",
      "  with 0.70000 as the threshold on privacy risk score, the precision-recall pair is (1.00000, 0.00140)\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.63558, 0.16220)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.57909, 0.87500)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=5\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.62834, 0.10820)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.57398, 0.87440)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=6\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.61623, 0.11240)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.57273, 0.70640)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=7\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.62541, 0.15360)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.57991, 0.77720)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=8\"\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.62054, 0.11120)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.55600, 0.81520)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CLASS=9\"\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.56808, 0.68920)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CORRECTLY_CLASSIFIED=True\"\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.53422, 0.43662)\n",
      "\n",
      "Privacy risk score analysis over slice: \"CORRECTLY_CLASSIFIED=False\"\n",
      "  with 0.70000 as the threshold on privacy risk score, the precision-recall pair is (0.71764, 0.35140)\n",
      "  with 0.60000 as the threshold on privacy risk score, the precision-recall pair is (0.68704, 0.64067)\n",
      "  with 0.50000 as the threshold on privacy risk score, the precision-recall pair is (0.64406, 0.84983)\n"
     ]
    }
   ],
   "source": [
    "# compute privacy risk scores on all given data slices\n",
    "risk_score_results = mia.run_privacy_risk_score_analysis(input,\n",
    "                                                         SlicingSpec(\n",
    "                                                             entire_dataset = True,\n",
    "                                                             by_class = True,\n",
    "                                                             by_classification_correctness = True))\n",
    "# print the summary of privacy risk score analysis\n",
    "print(risk_score_results.summary(threshold_list=[1, 0.9, 0.8, 0.7, 0.6, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "Membership inference privacy risk score codelab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
